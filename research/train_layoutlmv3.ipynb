{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shaur\\anaconda3\\envs\\glean\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "from network import dataset\n",
    "from src.Glean.utils.evaluate import evaluate\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from src.Glean import logger\n",
    "from src.Glean.entity.config_entity import TrainModelConfig\n",
    "from pathlib import Path\n",
    "from src.Glean.config.configuration import ConfigurationManager\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import LayoutLMv3Model\n",
    "\n",
    "from network.neighbour_attention import MultiHeadAttention\n",
    "from network.neighbour_embedding import NeighbourEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-21 00:42:24,064: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2023-11-21 00:42:24,064: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2023-11-21 00:42:24,073: INFO: common: created directory at: artifacts]\n",
      "[2023-11-21 00:42:24,073: INFO: common: created directory at: artifacts/best_model]\n"
     ]
    }
   ],
   "source": [
    "config = ConfigurationManager()\n",
    "train_config = config.train_model_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-21 00:42:24,083: INFO: dataset: Preprocessed data available, Loading data from cache...]\n",
      "\n",
      "Class Mapping: {'registration_num': 0}\n",
      "Classs counts: {'registration_num': 197}\n",
      "[2023-11-21 00:42:24,101: INFO: dataset: Preprocessed data available, Loading data from cache...]\n",
      "\n",
      "Class Mapping: {'registration_num': 0}\n",
      "Classs counts: {'registration_num': 99}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50268"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = dataset.DocumentsDataset(train_config, 'train')\n",
    "val_data = dataset.DocumentsDataset(train_config, 'val')\n",
    "\n",
    "VOCAB_SIZE = len(train_data.vocab)\n",
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, val_dataloader, epochs):\n",
    "\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        model = model.to(device)\n",
    "        # writer = SummaryWriter(comment=f\"LR_{self.config.lr}_BATCH_{self.config.batch_size}\")\n",
    "        # criterion = nn.BCELoss()\n",
    "        criterion = torch.nn.BCELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=train_config.lr)\n",
    "\n",
    "        train_loss_history = []\n",
    "        train_accuracy_history = []\n",
    "        recall_history = []\n",
    "        precision_history = []\n",
    "        f1_history = []\n",
    "        val_loss_history = []\n",
    "        val_accuracy_history = []\n",
    "        val_recall_history = []\n",
    "        val_precision_history = []\n",
    "        val_f1_history = []\n",
    "        val_max_score = 0.0\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "\n",
    "            train_loss = 0.0\n",
    "            train_accuracy = 0.0\n",
    "            y_preds = []\n",
    "            y_labels = []\n",
    "\n",
    "            for field, candidate, words, positions, masks, labels in tqdm(train_dataloader, desc=\"Epoch %s\" % epoch):\n",
    "                # print(field.dim())\n",
    "                field = field.to(device)\n",
    "                candidate = candidate.to(device)\n",
    "                words = words.to(device)\n",
    "                positions = positions.to(device)\n",
    "                masks = masks.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(field, candidate, words, positions, masks)\n",
    "                print(\"Outputs: \",outputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                print(\"Loss: \", loss)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                preds = outputs.round()\n",
    "                y_preds.extend(list(preds.cpu().detach().numpy().reshape(1, -1)[0]))\n",
    "                y_labels.extend(list(labels.cpu().detach().numpy().reshape(1, -1)[0]))\n",
    "\n",
    "                train_accuracy += torch.sum(preds == labels).item()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            # else:\n",
    "            #     val_accuracy, val_loss, val_recall, val_precision, val_f1 = evaluate(model, val_dataloader, criterion)\n",
    "\n",
    "            #     train_loss = train_loss / train_dataloader.sampler.num_samples\n",
    "            #     train_accuracy = train_accuracy / train_dataloader.sampler.num_samples\n",
    "            #     recall = recall_score(y_labels, y_preds)\n",
    "            #     precision = precision_score(y_labels, y_preds)\n",
    "            #     f1score = f1_score(y_labels, y_preds)\n",
    "\n",
    "            #     train_loss_history.append(train_loss)\n",
    "            #     train_accuracy_history.append(train_accuracy)\n",
    "            #     recall_history.append(recall)\n",
    "            #     precision_history.append(precision)\n",
    "            #     f1_history.append(f1score)\n",
    "            #     val_loss_history.append(val_loss)\n",
    "            #     val_accuracy_history.append(val_accuracy)\n",
    "            #     val_recall_history.append(val_recall)\n",
    "            #     val_precision_history.append(val_precision)\n",
    "            #     val_f1_history.append(val_f1)\n",
    "\n",
    "            #     if val_recall > val_max_score: # Saving the best model\n",
    "            #         print('saving model....')\n",
    "            #         val_max_score = val_recall\n",
    "            #         os.makedirs(Path(train_config.best_model_dir), exist_ok=True)\n",
    "            #         torch.save(model, Path(train_config.best_model_dir)/'model.pth')\n",
    "            #     print(f\"Metrics for Epoch {epoch}:  Loss:{round(train_loss, 4)} \\\n",
    "            #             Recall: {round(recall, 4)} \\\n",
    "            #             Validation Loss: {round(val_loss, 4)} \\\n",
    "            #             Validation Recall: {round(val_recall, 4)}\")\n",
    "            break\n",
    "\n",
    "        return {\n",
    "            # 'training_loss': train_loss_history,\n",
    "            # 'training_accuracy': train_accuracy_history,\n",
    "            'training_recall': recall_history,\n",
    "            'training_precision': precision_history,\n",
    "            'training_f1': f1_history,\n",
    "            # 'validation_loss': val_loss_history,\n",
    "            # 'validation_accuracy': val_accuracy_history,\n",
    "            'validation_recall': val_recall_history,\n",
    "            'validation_precision': val_precision_history,\n",
    "            'validation_f1': val_f1_history\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, neighbours, heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lm_model = LayoutLMv3Model.from_pretrained('nielsr/layoutlmv3-finetuned-funsd')\n",
    "\n",
    "        self.cand_embed = nn.Linear(2, 128)\n",
    "        self.field_embed = nn.Linear(768, embedding_dim)\n",
    "        self.embedding_dimension = embedding_dim\n",
    "        self.neighbour_embeddings = NeighbourEmbedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.attention_encodings = MultiHeadAttention(heads, embedding_dim * 2)\n",
    "        self.linear_projection = nn.Linear(neighbours * embedding_dim * 2, 4 * embedding_dim * 2)\n",
    "        self.linear_projection_2 = nn.Linear(128 + (2 * embedding_dim), embedding_dim)\n",
    "        self.cos_sim = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "\n",
    "    def forward(self, field_id, candidate, neighbour_words, neighbour_positions, masks):\n",
    "        # Field and candidate embeddings\n",
    "        # print(\"Field_id.shape: \", field_id.shape)\n",
    "        id_lm_embed = self.lm_model.embeddings.word_embeddings(field_id.view(-1).to(torch.long))\n",
    "        # print(\"ID lm embed shape: \", id_lm_embed.shape)\n",
    "        id_embed = self.field_embed(id_lm_embed)\n",
    "        # print(\"ID Embed shape: \", id_embed.shape)\n",
    "        cand_embed = self.cand_embed(candidate)\n",
    "\n",
    "        # Neighbour embeddings\n",
    "        neighbour_embeds = self.neighbour_embeddings(neighbour_words, neighbour_positions)\n",
    "        # print(neighbour_embeds)\n",
    "\n",
    "        # Attention encodings\n",
    "        self_attention = self.attention_encodings(neighbour_embeds, neighbour_embeds, neighbour_embeds, mask=masks)\n",
    "\n",
    "        # Linear projection of attention to concatenate with candidate embedding\n",
    "        bs = self_attention.size(0)\n",
    "        self_attention = self_attention.view(bs, -1)\n",
    "        linear_proj = F.relu(self.linear_projection(self_attention))\n",
    "\n",
    "        linear_proj = linear_proj.view(bs, 4, -1)\n",
    "\n",
    "        pooled_attention = F.max_pool2d(linear_proj, 2, 2)\n",
    "\n",
    "        unrolled_attention = pooled_attention.view(bs, -1)\n",
    "\n",
    "        # Concatenating Candidate embedding and Attention\n",
    "        concat = torch.cat((cand_embed, unrolled_attention), dim=1)\n",
    "\n",
    "        # Re-projecting concatenated embedding to calculate cosing similarity\n",
    "        projected_candidate_encoding = F.relu(self.linear_projection_2(concat))\n",
    "        # print(\"Projected candidate encoding shape: \",projected_candidate_encoding.shape)\n",
    "        # print(\"ID embed shape: \", id_embed.shape)\n",
    "        # Calculating cosine similarity and scaling to [0,1]\n",
    "        similarity = self.cos_sim(id_embed, projected_candidate_encoding).view(bs, -1)\n",
    "        scores = (similarity + 1) / 2\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/1376 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5])\n",
      "Outputs:  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/1376 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\shaur\\Desktop\\Glean_Implementation\\research\\train_layoutlmv3.ipynb Cell 7\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shaur/Desktop/Glean_Implementation/research/train_layoutlmv3.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m val_loader \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mDataLoader(val_data, batch_size\u001b[39m=\u001b[39mtrain_config\u001b[39m.\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shaur/Desktop/Glean_Implementation/research/train_layoutlmv3.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model \u001b[39m=\u001b[39m Model(VOCAB_SIZE, train_config\u001b[39m.\u001b[39membedding_size, train_config\u001b[39m.\u001b[39mneighbours, train_config\u001b[39m.\u001b[39mheads)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/shaur/Desktop/Glean_Implementation/research/train_layoutlmv3.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m train_model \u001b[39m=\u001b[39m train(model, train_loader, val_loader, train_config\u001b[39m.\u001b[39;49mepochs)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shaur/Desktop/Glean_Implementation/research/train_layoutlmv3.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# train_fn = train_model.train()\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shaur/Desktop/Glean_Implementation/research/train_layoutlmv3.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# train_fn\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\shaur\\Desktop\\Glean_Implementation\\research\\train_layoutlmv3.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shaur/Desktop/Glean_Implementation/research/train_layoutlmv3.ipynb#W6sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shaur/Desktop/Glean_Implementation/research/train_layoutlmv3.ipynb#W6sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(field, candidate, words, positions, masks)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/shaur/Desktop/Glean_Implementation/research/train_layoutlmv3.ipynb#W6sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mprint\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mOutputs: \u001b[39;49m\u001b[39m\"\u001b[39;49m,outputs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shaur/Desktop/Glean_Implementation/research/train_layoutlmv3.ipynb#W6sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shaur/Desktop/Glean_Implementation/research/train_layoutlmv3.ipynb#W6sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLoss: \u001b[39m\u001b[39m\"\u001b[39m, loss)\n",
      "File \u001b[1;32mc:\\Users\\shaur\\anaconda3\\envs\\glean\\lib\\site-packages\\torch\\_tensor.py:431\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[1;34m(self, tensor_contents)\u001b[0m\n\u001b[0;32m    427\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    428\u001b[0m         Tensor\u001b[39m.\u001b[39m\u001b[39m__repr__\u001b[39m, (\u001b[39mself\u001b[39m,), \u001b[39mself\u001b[39m, tensor_contents\u001b[39m=\u001b[39mtensor_contents\n\u001b[0;32m    429\u001b[0m     )\n\u001b[0;32m    430\u001b[0m \u001b[39m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[1;32m--> 431\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_tensor_str\u001b[39m.\u001b[39;49m_str(\u001b[39mself\u001b[39;49m, tensor_contents\u001b[39m=\u001b[39;49mtensor_contents)\n",
      "File \u001b[1;32mc:\\Users\\shaur\\anaconda3\\envs\\glean\\lib\\site-packages\\torch\\_tensor_str.py:664\u001b[0m, in \u001b[0;36m_str\u001b[1;34m(self, tensor_contents)\u001b[0m\n\u001b[0;32m    662\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad(), torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39m_python_dispatch\u001b[39m.\u001b[39m_disable_current_modes():\n\u001b[0;32m    663\u001b[0m     guard \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_DisableFuncTorch()\n\u001b[1;32m--> 664\u001b[0m     \u001b[39mreturn\u001b[39;00m _str_intern(\u001b[39mself\u001b[39;49m, tensor_contents\u001b[39m=\u001b[39;49mtensor_contents)\n",
      "File \u001b[1;32mc:\\Users\\shaur\\anaconda3\\envs\\glean\\lib\\site-packages\\torch\\_tensor_str.py:595\u001b[0m, in \u001b[0;36m_str_intern\u001b[1;34m(inp, tensor_contents)\u001b[0m\n\u001b[0;32m    593\u001b[0m                     tensor_str \u001b[39m=\u001b[39m _tensor_str(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_dense(), indent)\n\u001b[0;32m    594\u001b[0m                 \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 595\u001b[0m                     tensor_str \u001b[39m=\u001b[39m _tensor_str(\u001b[39mself\u001b[39;49m, indent)\n\u001b[0;32m    597\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayout \u001b[39m!=\u001b[39m torch\u001b[39m.\u001b[39mstrided:\n\u001b[0;32m    598\u001b[0m     suffixes\u001b[39m.\u001b[39mappend(\u001b[39m\"\u001b[39m\u001b[39mlayout=\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayout))\n",
      "File \u001b[1;32mc:\\Users\\shaur\\anaconda3\\envs\\glean\\lib\\site-packages\\torch\\_tensor_str.py:347\u001b[0m, in \u001b[0;36m_tensor_str\u001b[1;34m(self, indent)\u001b[0m\n\u001b[0;32m    343\u001b[0m     \u001b[39mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[0;32m    344\u001b[0m         \u001b[39mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[0;32m    345\u001b[0m     )\n\u001b[0;32m    346\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 347\u001b[0m     formatter \u001b[39m=\u001b[39m _Formatter(get_summarized_data(\u001b[39mself\u001b[39;49m) \u001b[39mif\u001b[39;49;00m summarize \u001b[39melse\u001b[39;49;00m \u001b[39mself\u001b[39;49m)\n\u001b[0;32m    348\u001b[0m     \u001b[39mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[39mself\u001b[39m, indent, summarize, formatter)\n",
      "File \u001b[1;32mc:\\Users\\shaur\\anaconda3\\envs\\glean\\lib\\site-packages\\torch\\_tensor_str.py:138\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_width \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_width, \u001b[39mlen\u001b[39m(value_str))\n\u001b[0;32m    136\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    137\u001b[0m     nonzero_finite_vals \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmasked_select(\n\u001b[1;32m--> 138\u001b[0m         tensor_view, torch\u001b[39m.\u001b[39;49misfinite(tensor_view) \u001b[39m&\u001b[39m tensor_view\u001b[39m.\u001b[39mne(\u001b[39m0\u001b[39m)\n\u001b[0;32m    139\u001b[0m     )\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m nonzero_finite_vals\u001b[39m.\u001b[39mnumel() \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    142\u001b[0m         \u001b[39m# no valid number, do nothing\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "train_loader = data.DataLoader(train_data, batch_size=train_config.batch_size, shuffle=True)\n",
    "val_loader = data.DataLoader(val_data, batch_size=train_config.batch_size, shuffle=True)\n",
    "\n",
    "model = Model(VOCAB_SIZE, train_config.embedding_size, train_config.neighbours, train_config.heads)\n",
    "train_model = train(model, train_loader, val_loader, train_config.epochs)\n",
    "# train_fn = train_model.train()\n",
    "# train_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LayoutLMv3Model(\n",
       "  (embeddings): LayoutLMv3TextEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (x_position_embeddings): Embedding(1024, 128)\n",
       "    (y_position_embeddings): Embedding(1024, 128)\n",
       "    (h_position_embeddings): Embedding(1024, 128)\n",
       "    (w_position_embeddings): Embedding(1024, 128)\n",
       "  )\n",
       "  (patch_embed): LayoutLMv3PatchEmbeddings(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (encoder): LayoutLMv3Encoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x LayoutLMv3Layer(\n",
       "        (attention): LayoutLMv3Attention(\n",
       "          (self): LayoutLMv3SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): LayoutLMv3SelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): LayoutLMv3Intermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): LayoutLMv3Output(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (rel_pos_bias): Linear(in_features=32, out_features=12, bias=False)\n",
       "    (rel_pos_x_bias): Linear(in_features=64, out_features=12, bias=False)\n",
       "    (rel_pos_y_bias): Linear(in_features=64, out_features=12, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import LayoutLMv3Model\n",
    "\n",
    "# Load the pretrained LayoutLMv3 model\n",
    "model = LayoutLMv3Model.from_pretrained('nielsr/layoutlmv3-finetuned-funsd')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

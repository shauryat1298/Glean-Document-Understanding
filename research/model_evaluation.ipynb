{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\shaur\\\\Desktop\\\\Glean_Implementation'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bash commands\n",
    "\n",
    "set MLFLOW_TRACKING_URI=https://dagshub.com/shauryat1298/Glean-Document-Understanding.mlflow\n",
    "\n",
    "set MLFLOW_TRACKING_USERNAME=shauryat1298\n",
    "\n",
    "set MLFLOW_TRACKING_PASSWORD=c5677632360a9e2d8981b4b2bf74d353830f2bc3\n",
    "\n",
    "python script.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MLFLOW_TRACKING_URI\"] = \"https://dagshub.com/shauryat1298/Glean-Document-Understanding.mlflow\"\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = \"shauryat1298\"\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = \"c5677632360a9e2d8981b4b2bf74d353830f2bc3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Glean.config.configuration import ConfigurationManager\n",
    "from src.Glean.components.train import TrainModel\n",
    "from network import dataset\n",
    "from pathlib import Path\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from urllib.parse import urlparse\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_dataloader, criterion):\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    val_accuracy = 0.0\n",
    "    val_loss = 0.0\n",
    "    y_preds = []\n",
    "    y_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_field, val_candidate, val_words, val_positions, masks, val_labels in val_dataloader:\n",
    "            val_field = val_field.to(device)\n",
    "            val_candidate = val_candidate.to(device)\n",
    "            val_words = val_words.to(device)\n",
    "            val_positions = val_positions.to(device)\n",
    "            masks = masks.to(device)\n",
    "            val_labels = val_labels.to(device)\n",
    "\n",
    "            val_outputs = model(val_field, val_candidate, val_words, val_positions, masks)\n",
    "            validation_loss = criterion(val_outputs, val_labels)\n",
    "\n",
    "            val_preds = val_outputs.round()\n",
    "            y_preds.extend(list(val_preds.cpu().detach().numpy().reshape(1, -1)[0]))\n",
    "            y_labels.extend(list(val_labels.cpu().detach().numpy().reshape(1, -1)[0]))\n",
    "\n",
    "            val_accuracy += torch.sum(val_preds == val_labels).item()\n",
    "            val_loss += validation_loss.item()\n",
    "\n",
    "        val_loss = val_loss / val_dataloader.sampler.num_samples\n",
    "        val_accuracy = val_accuracy / val_dataloader.sampler.num_samples\n",
    "        recall = recall_score(y_labels, y_preds)\n",
    "        precision = precision_score(y_labels, y_preds)\n",
    "        f1 = f1_score(y_labels, y_preds)\n",
    "\n",
    "    return val_accuracy, val_loss, recall, precision, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-04 03:32:11,998: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2023-11-04 03:32:12,000: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2023-11-04 03:32:12,000: INFO: common: created directory at: artifacts]\n",
      "[2023-11-04 03:32:12,001: INFO: common: created directory at: artifacts/train_model]\n",
      "[2023-11-04 03:32:12,002: INFO: dataset: Preprocessed data available, Loading data from cache...]\n",
      "\n",
      "Class Mapping: {'registration_num': 0}\n",
      "Classs counts: {'registration_num': 99}\n",
      "Test Accuracy: 0.8125904486251809 Test Loss: 0.12127573110063644 Test Recall: 0.8787878787878788 Test Precision 0.26047904191616766 Test F1 0.4018475750577367\n"
     ]
    }
   ],
   "source": [
    "config = ConfigurationManager()\n",
    "config = config.train_model_config()\n",
    "\n",
    "doc_data = dataset.DocumentsDataset(config=config, split_name='val')\n",
    "VOCAB_SIZE = len(doc_data.vocab)\n",
    "test_data = data.DataLoader(doc_data, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "# rlie = Model(VOCAB_SIZE, config.EMBEDDING_SIZE, config.NEIGHBOURS, config.HEADS)\n",
    "criterion = torch.nn.BCELoss()\n",
    "relie = torch.load(Path(config.best_model_dir)/\"model.pth\")\n",
    "# criterion = FocalLoss(alpha=2, gamma=5)\n",
    "\n",
    "test_accuracy, test_loss, test_recall, test_precision, test_f1 = evaluate(relie, test_data, criterion)\n",
    "\n",
    "mlflow.set_registry_uri(config.mlflow_uri)\n",
    "tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_params(config.all_params)\n",
    "    mlflow.log_metrics({\n",
    "        \"Test Accuracy\": test_accuracy,\n",
    "        \"Test Loss\": test_loss,\n",
    "        \"Test Recall\": test_recall,\n",
    "        \"Test Precision\": test_precision,\n",
    "        \"Test F1\": test_f1\n",
    "    })\n",
    "    if tracking_url_type_store != \"file\":\n",
    "        mlflow.pytorch(relie, \"model\", registered_model_name=\"TransformerModel\")\n",
    "    else:\n",
    "        mlflow.pytorch.log_model(relie, \"\")\n",
    "print(f\"Test Accuracy: {test_accuracy} Test Loss: {test_loss} Test Recall: {test_recall} Test Precision {test_precision} Test F1 {test_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
